{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtqQ27pZn6h0"
   },
   "source": [
    "# Customer Segmentation: A Systematic Approach from Raw Data to Actionable Insights\n",
    "\n",
    "### 1. Project Objective & Methodology\n",
    "\n",
    "The goal of this project is to perform an end-to-end customer segmentation analysis on the raw Online Retail dataset. We will systematically process and transform the raw transactional data into a rich, multi-dimensional customer profile. This profile will then be used with unsupervised machine learning to identify distinct, actionable customer segments.\n",
    "\n",
    "Our methodology emphasizes a rigorous and reproducible workflow:\n",
    "\n",
    "1.  **Environment Setup:** Import all necessary libraries and configure the notebook environment for consistency.\n",
    "2.  **Data Ingestion:** Load the raw dataset from its source.\n",
    "3.  **Data Cleaning & Preprocessing:** Rigorously clean the raw data to handle missing values, duplicates, and invalid entries, creating a reliable analytical base.\n",
    "4.  **Advanced Feature Engineering:** Move beyond traditional RFM by creating a custom 8-feature **\"Customer DNA\"** profile that captures profitability, loyalty, and purchasing behavior.\n",
    "5.  **Exploratory Data Analysis (EDA):** Thoroughly analyze and visualize the engineered features to understand their statistical properties and relationships.\n",
    "6.  **Modeling & Segmentation:** Apply and evaluate clustering algorithms to segment the customer base.\n",
    "\n",
    "This notebook will serve as a definitive guide, with each step clearly documented to ensure clarity and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1762285743589,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "6y8BdiJQN6hI",
    "outputId": "03ffc571-d2ed-45b8-b176-700e4c4b1c52"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 1: Environment Setup\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1.1 Core Libraries for Data Manipulation ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "# --- 1.2 Libraries for Data Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# --- 1.3 Libraries for Machine Learning ---\n",
    "# Preprocessing & Dimensionality Reduction\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Modeling (to be used in the modeling phase)\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# --- 1.4 Configure Notebook Environment ---\n",
    "# Set a consistent, professional style for all plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "# Suppress scientific notation for floats for better readability\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "# Set default figure size\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"✅ All libraries imported and environment configured successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-oB_sX1odKI"
   },
   "source": [
    "### 2. Data Ingestion\n",
    "\n",
    "In this step, we will acquire the raw dataset. We are working exclusively from the `OnlineRetail.xlsx` file, ensuring our entire analysis is built from the ground up from a single source of truth.\n",
    "\n",
    "* **What it does:** It uses the `gdown` library to download the raw data file directly from its shared Google Drive link.\n",
    "* **Why we do this:** This automates the data acquisition process, making the notebook self-contained and easily reproducible by any team member or professor. After downloading, it loads the data into a pandas DataFrame for initial inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "executionInfo": {
     "elapsed": 130011,
     "status": "ok",
     "timestamp": 1762285883909,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "ek3k8FgOOSc2",
    "outputId": "0ae6c9a8-4215-407a-af53-5b33a0a52f58"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 2: Data Ingestion - Downloading & Loading the Raw File\n",
    "# =============================================================================\n",
    "\n",
    "# --- 2.1 Install gdown for Google Drive Downloads ---\n",
    "# We run this quietly as it only needs to happen once per session\n",
    "!pip install gdown --quiet\n",
    "\n",
    "# --- 2.2 Download the Raw Dataset ---\n",
    "# The file ID for the 'OnlineRetail.xlsx' Google Sheet\n",
    "file_id = '17hbtFKg70j5x_QN8F0dBZmsHxwkFKIGawmQ9_uAfFU0'\n",
    "output_filename = 'OnlineRetail.xlsx'\n",
    "\n",
    "print(f\"⬇️ Downloading raw data file: '{output_filename}'...\")\n",
    "try:\n",
    "    # gdown automatically detects a Google Sheet and downloads it as an .xlsx file\n",
    "    !gdown $file_id -O $output_filename\n",
    "    print(f\"✅ File downloaded successfully as '{output_filename}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: File download failed. Please check the file ID and sharing permissions.\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# --- 2.3 Load the Dataset into a DataFrame ---\n",
    "try:\n",
    "    df_raw = pd.read_excel(output_filename)\n",
    "    print(f\"\\n✅ Raw dataset loaded successfully.\")\n",
    "    print(f\"Shape of the dataset: {df_raw.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: '{output_filename}' not found.\")\n",
    "    print(\"Please ensure the file was downloaded correctly in the step above.\")\n",
    "\n",
    "# --- 2.4 Initial Data Inspection ---\n",
    "print(\"\\n--- First 5 Rows of the Raw Data ---\")\n",
    "display(df_raw.head())\n",
    "\n",
    "print(\"\\n--- Data Types and Missing Values ---\")\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjhk2wu5cdoy"
   },
   "source": [
    "### 3. Data Cleaning & Preprocessing\n",
    "\n",
    "This is the most critical step of the analysis. Raw transactional data is notoriously \"dirty.\" Before we can perform any feature engineering, we must create a clean, reliable analytical base. Our process will be systematic:\n",
    "\n",
    "* **Handle Missing `CustomerID`s:** Transactions without a `CustomerID` are \"guest checkouts.\" They are unusable for building customer profiles, so these rows must be removed.\n",
    "* **Handle Duplicates:** We will check for and remove any exact duplicate rows to prevent double-counting.\n",
    "* **Handle Data Types:** We will convert `CustomerID` from a float to a string (object) for proper grouping.\n",
    "* **Handle Invalid Transactions:** We will filter out transactions with a `UnitPrice` of $0, as these do not represent a valid sale.\n",
    "* **Separate Purchases vs. Cancellations:** We will create two distinct DataFrames:\n",
    "    1.  `df_purchases`: All transactions with `Quantity > 0`. This will be our main dataset for analysis.\n",
    "    2.  `df_cancellations`: All transactions where `InvoiceNo` starts with 'C'. We will keep this data to engineer our advanced `CancellationRate` feature later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3430,
     "status": "ok",
     "timestamp": 1762286401175,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "AUZ1y4n1cexp",
    "outputId": "b644aa8e-760e-49f4-a800-38ce523001a2"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 3: Data Cleaning & Preprocessing\n",
    "# =============================================================================\n",
    "\n",
    "# We will work from the df_raw DataFrame loaded in Block 2\n",
    "\n",
    "# --- 3.1 Handle Missing Values (CustomerID) ---\n",
    "# We cannot segment customers we cannot identify.\n",
    "initial_rows = df_raw.shape[0]\n",
    "print(f\"Initial row count: {initial_rows:,}\")\n",
    "\n",
    "# Check for missing CustomerIDs\n",
    "missing_customers = df_raw['CustomerID'].isna().sum()\n",
    "print(f\"Rows with missing CustomerID: {missing_customers:,} ({missing_customers/initial_rows:.1%})\")\n",
    "\n",
    "# Drop rows where CustomerID is null\n",
    "df_cleaned = df_raw.dropna(subset=['CustomerID'])\n",
    "print(f\"Row count after dropping missing CustomerIDs: {df_cleaned.shape[0]:,}\")\n",
    "\n",
    "# --- 3.2 Convert CustomerID to String ---\n",
    "# CustomerID is a categorical identifier, not a numeric value.\n",
    "# We convert it to string and remove the '.0' float suffix.\n",
    "df_cleaned['CustomerID'] = df_cleaned['CustomerID'].astype(float).astype(int).astype(str)\n",
    "print(\"\\n✅ CustomerID converted to string type.\")\n",
    "\n",
    "# --- 3.3 Remove Duplicates ---\n",
    "# Check for and drop any exact duplicate rows\n",
    "initial_rows = df_cleaned.shape[0]\n",
    "duplicates = df_cleaned.duplicated().sum()\n",
    "print(f\"\\nFound {duplicates:,} duplicate rows.\")\n",
    "\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "print(f\"Row count after dropping duplicates: {df_cleaned.shape[0]:,}\")\n",
    "\n",
    "# --- 3.4 Handle Invalid UnitPrice ---\n",
    "# Filter out transactions with a UnitPrice of 0 or less\n",
    "invalid_price = (df_cleaned['UnitPrice'] <= 0).sum()\n",
    "print(f\"\\nFound {invalid_price:,} rows with UnitPrice <= 0.\")\n",
    "\n",
    "df_cleaned = df_cleaned[df_cleaned['UnitPrice'] > 0]\n",
    "print(f\"Row count after dropping invalid UnitPrice: {df_cleaned.shape[0]:,}\")\n",
    "\n",
    "# --- 3.5 Separate Purchases and Cancellations ---\n",
    "# This is a key step for our advanced feature engineering.\n",
    "\n",
    "# 3.5.1 Create df_cancellations\n",
    "# These are all transactions where the InvoiceNo starts with 'C'\n",
    "df_cancellations = df_cleaned[df_cleaned['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "print(f\"\\nFound {df_cancellations.shape[0]:,} cancellation entries.\")\n",
    "\n",
    "# 3.5.2 Create df_purchases\n",
    "# These are all non-cancellation transactions with a Quantity > 0\n",
    "df_purchases = df_cleaned[~df_cleaned['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "df_purchases = df_purchases[df_purchases['Quantity'] > 0]\n",
    "print(f\"Found {df_purchases.shape[0]:,} valid purchase entries.\")\n",
    "\n",
    "# --- 3.6 Final Check ---\n",
    "print(\"\\n✅ Data cleaning complete.\")\n",
    "print(\"\\n--- Purchase Data Info ---\")\n",
    "df_purchases.info()\n",
    "\n",
    "print(\"\\n--- Cancellation Data Info ---\")\n",
    "df_cancellations.info()\n",
    "\n",
    "print(\"\\n--- First 5 Rows of Cleaned Purchase Data ---\")\n",
    "display(df_purchases.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki5UWSTmpL_6"
   },
   "source": [
    "### 4. Advanced Feature Engineering\n",
    "\n",
    "This is the most valuable step in our analysis. We will transform the cleaned, transactional data (one row per *item*) into a customer-centric dataset (one row per *customer*).\n",
    "\n",
    "This new table will contain our 8-feature **\"Customer DNA\" profile**, which goes far beyond a simple RFM (Recency, Frequency, Monetary) model.\n",
    "\n",
    "Our 8 features are designed to answer three key questions about each customer:\n",
    "\n",
    "1.  **Transactional Behavior (What & How):**\n",
    "    * `Recency`: How recently did they buy?\n",
    "    * `TotalOrders`: How many orders have they placed?\n",
    "    * `TotalMonetary`: How much have they spent in total?\n",
    "    * `AverageOrderValue`: Do they place big, high-value orders?\n",
    "    * `AverageBasketSize`: Do they buy many items at once?\n",
    "    * `ProductDiversity`: Do they buy a wide variety of products?\n",
    "\n",
    "2.  **Loyalty (How long):**\n",
    "    * `CustomerTenure`: How long have they been a customer? (A new customer with 5 orders is very different from a 3-year-old customer with 5 orders).\n",
    "\n",
    "3.  **Profitability (How reliable):**\n",
    "    * `CancellationRate`: What percentage of their orders do they cancel? (A high-spending but high-cancellation customer is not profitable).\n",
    "\n",
    "We will calculate these features using our `df_purchases` and `df_cancellations` dataframes and then merge them into one final `df_final` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 855
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1762286557202,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "7G7HZtCzRtmW",
    "outputId": "450200d9-c7f2-4411-fd30-947a24bdb078"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 4: Advanced Feature Engineering\n",
    "# =============================================================================\n",
    "\n",
    "# We will use df_purchases, df_cancellations, and df_cleaned from Block 3\n",
    "\n",
    "# --- 4.1 Create TotalPrice Feature ---\n",
    "# This is a prerequisite for all monetary calculations\n",
    "df_purchases['TotalPrice'] = df_purchases['Quantity'] * df_purchases['UnitPrice']\n",
    "\n",
    "# --- 4.2 Define Snapshot Date ---\n",
    "# This is our reference point for \"today\" to calculate Recency and Tenure\n",
    "snapshot_date = df_purchases['InvoiceDate'].max() + dt.timedelta(days=1)\n",
    "print(f\"Snapshot date defined as: {snapshot_date.date()}\")\n",
    "\n",
    "# --- 4.3 Aggregate Purchase & Loyalty Features ---\n",
    "print(\"Calculating purchase and loyalty features...\")\n",
    "# We can get most of our features in a single aggregation from df_purchases\n",
    "df_agg = df_purchases.groupby('CustomerID').agg(\n",
    "    TotalMonetary=('TotalPrice', 'sum'),          # For TotalMonetary\n",
    "    TotalOrders=('InvoiceNo', 'nunique'),       # For TotalOrders\n",
    "    ProductDiversity=('StockCode', 'nunique'),    # For ProductDiversity\n",
    "    TotalQuantity=('Quantity', 'sum'),          # To calculate AverageBasketSize\n",
    "    LastPurchaseDate=('InvoiceDate', 'max'),    # To calculate Recency\n",
    "    FirstPurchaseDate=('InvoiceDate', 'min')    # To calculate CustomerTenure\n",
    ").reset_index()\n",
    "\n",
    "# --- 4.4 Aggregate Profitability Features ---\n",
    "print(\"Calculating profitability features...\")\n",
    "# Get total invoices from the *original* cleaned dataframe\n",
    "df_total_invoices = df_cleaned.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
    "df_total_invoices.rename(columns={'InvoiceNo': 'TotalInvoices'}, inplace=True)\n",
    "\n",
    "# Get cancelled invoices from the cancellations dataframe\n",
    "df_cancelled_invoices = df_cancellations.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
    "df_cancelled_invoices.rename(columns={'InvoiceNo': 'CancelledInvoices'}, inplace=True)\n",
    "\n",
    "# --- 4.5 Assemble Final \"Customer DNA\" Table ---\n",
    "print(\"Assembling final feature table...\")\n",
    "# 1. Start with our aggregated purchase data\n",
    "df_final = df_agg.copy()\n",
    "# 2. Merge total invoice counts\n",
    "df_final = pd.merge(df_final, df_total_invoices, on='CustomerID', how='left')\n",
    "# 3. Merge cancelled invoice counts\n",
    "df_final = pd.merge(df_final, df_cancelled_invoices, on='CustomerID', how='left')\n",
    "# 4. FillNa for customers with 0 cancellations\n",
    "df_final['CancelledInvoices'].fillna(0, inplace=True)\n",
    "\n",
    "# --- 4.6 Calculate the 8 Final Features ---\n",
    "print(\"Calculating all 8 'Customer DNA' features...\")\n",
    "\n",
    "# 1. Recency (days since last purchase)\n",
    "df_final['Recency'] = (snapshot_date - df_final['LastPurchaseDate']).dt.days\n",
    "\n",
    "# 2. CustomerTenure (days since first purchase)\n",
    "df_final['CustomerTenure'] = (snapshot_date - df_final['FirstPurchaseDate']).dt.days\n",
    "\n",
    "# 3. CancellationRate (profitability metric)\n",
    "df_final['CancellationRate'] = df_final['CancelledInvoices'] / df_final['TotalInvoices']\n",
    "\n",
    "# 4. AverageOrderValue (AOV)\n",
    "df_final['AverageOrderValue'] = df_final['TotalMonetary'] / df_final['TotalOrders']\n",
    "\n",
    "# 5. AverageBasketSize (items per order)\n",
    "df_final['AverageBasketSize'] = df_final['TotalQuantity'] / df_final['TotalOrders']\n",
    "\n",
    "# The other 3 features (TotalMonetary, TotalOrders, ProductDiversity) are already in the table.\n",
    "\n",
    "# --- 4.7 Final Selection & Inspection ---\n",
    "# Select our 8 features + CustomerID\n",
    "feature_columns = [\n",
    "    'Recency',\n",
    "    'CustomerTenure',\n",
    "    'TotalOrders',\n",
    "    'TotalMonetary',\n",
    "    'AverageOrderValue',\n",
    "    'AverageBasketSize',\n",
    "    'ProductDiversity',\n",
    "    'CancellationRate'\n",
    "]\n",
    "\n",
    "df_final = df_final[['CustomerID'] + feature_columns]\n",
    "\n",
    "# Handle any NaNs/Infs from division by zero (e.g., if TotalOrders was 0)\n",
    "df_final.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_final.dropna(inplace=True)\n",
    "\n",
    "print(f\"\\n✅ Feature engineering complete. Final customer profile shape: {df_final.shape}\")\n",
    "print(f\"   (Found {df_final.shape[0]:,} customers with at least one valid purchase)\")\n",
    "\n",
    "print(\"\\n--- First 5 Rows of 'Customer DNA' Table ---\")\n",
    "display(df_final.head())\n",
    "\n",
    "print(\"\\n--- Descriptive Statistics of Final Features ---\")\n",
    "display(df_final.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5KGI_59pZB8"
   },
   "source": [
    "### 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section performs a comprehensive exploratory analysis of the 8-feature \"Customer DNA\" profile we engineered in the previous block.\n",
    "\n",
    "#### 5.1 EDA Objectives\n",
    "\n",
    "The primary objectives of this analysis are to:\n",
    "1.  **Validate Feature Properties:** Quantify the statistical properties of each feature (central tendency, spread, and range).\n",
    "2.  **Diagnose Data Quality:** Visually and statistically identify the presence of outliers and, critically, the **degree of skewness** in each feature's distribution.\n",
    "3.  **Analyze Relationships:** Investigate the correlations and bivariate relationships between features to understand their interdependencies.\n",
    "4.  **Assess Cluster Readiness:** Use advanced, non-linear dimensionality reduction (t-SNE) to project the 8-dimensional data into a visual \"map,\" providing an initial assessment of natural, separable clusters.\n",
    "\n",
    "#### 5.2 Diagnostic Expectation (Key Finding)\n",
    "\n",
    "It is hypothesized that our raw, engineered features (especially those related to monetary value and time) will be **highly right-skewed** and contain **significant outliers**.\n",
    "\n",
    "The following plots (histograms, box plots) are designed to *confirm* and *quantify* this hypothesis. Observing this severe skew is not an error; it is the **critical diagnostic finding** of our EDA. It provides the empirical justification for the **mandatory logarithmic transformation** and **standard scaling** in the next preprocessing step, which are required for our distance-based clustering algorithms to function correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 34297,
     "status": "ok",
     "timestamp": 1762287323795,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "NI-neDitR-xQ",
    "outputId": "77a783c7-a4d1-4ae5-c0ea-ca0a97dfb4f9"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 5: Exploratory Data Analysis (EDA)\n",
    "# =============================================================================\n",
    "\n",
    "# We will analyze the df_final DataFrame created in Block 4\n",
    "df_eda = df_final.drop('CustomerID', axis=1)\n",
    "\n",
    "# --- 5.1 Descriptive & Skewness Analysis ---\n",
    "print(\"--- 5.1 Descriptive & Skewness Analysis ---\")\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "desc_stats = df_eda.describe().T\n",
    "display(desc_stats)\n",
    "\n",
    "print(\"\\nSkewness of Features:\")\n",
    "# Skewness > 1 or < -1 indicates high skew\n",
    "skew_df = pd.DataFrame(df_eda.skew(), columns=['Skewness'])\n",
    "display(skew_df)\n",
    "\n",
    "# --- 5.2 Distribution Analysis (Histograms) ---\n",
    "print(\"\\n--- 5.2 Generating Distribution (Histograms) ---\")\n",
    "# This figure is now *only* for histograms, making them larger.\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.suptitle(\"Histograms of Feature Distributions (Before Transformation)\", fontsize=18)\n",
    "for i, feature in enumerate(df_eda.columns):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(df_eda[feature], bins=50, kde=False)\n",
    "    plt.title(f'Distribution of {feature}', fontsize=12)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(\"feature_histograms.png\")\n",
    "print(\"✅ feature_histograms.png saved.\")\n",
    "plt.show() # This displays the plot and creates a \"gap\"\n",
    "\n",
    "# --- 5.3 Outlier Analysis (Box Plots) ---\n",
    "print(\"\\n--- 5.3 Generating Outlier Analysis (Box Plots) ---\")\n",
    "# A *new*, separate figure for box plots.\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.suptitle(\"Box Plots of Feature Distributions (Showing Outliers)\", fontsize=18)\n",
    "for i, feature in enumerate(df_eda.columns):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.boxplot(y=df_eda[feature])\n",
    "    plt.title(f'Box Plot of {feature}', fontsize=12)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(\"feature_boxplots.png\")\n",
    "print(\"✅ feature_boxplots.png saved.\")\n",
    "plt.show() # Creates another \"gap\"\n",
    "\n",
    "# --- 5.4 Relationship Analysis (Heatmap & Pairplot) ---\n",
    "print(\"\\n--- 5.4 Generating Relationship Analysis ---\")\n",
    "\n",
    "# A separate figure for the Correlation Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = df_eda.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of the 8 Features', fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_correlation_heatmap.png\")\n",
    "print(\"✅ feature_correlation_heatmap.png saved.\")\n",
    "plt.show() # Creates another \"gap\"\n",
    "\n",
    "# A separate figure for the Targeted Pairplot\n",
    "print(\"Generating targeted pairplot...\")\n",
    "pairplot_features = ['Recency', 'TotalMonetary', 'CustomerTenure', 'CancellationRate']\n",
    "pairplot_fig = sns.pairplot(df_eda[pairplot_features], plot_kws={'alpha': 0.4, 's': 10})\n",
    "pairplot_fig.fig.suptitle(\"Targeted Pairplot of Key Features\", y=1.02, fontsize=16)\n",
    "plt.savefig(\"targeted_pairplot.png\")\n",
    "print(\"✅ targeted_pairplot.png saved.\")\n",
    "plt.show() # Creates another \"gap\"\n",
    "\n",
    "# --- 5.5 Cluster Readiness Check (t-SNE) ---\n",
    "print(\"\\n--- 5.5 Running t-SNE for Cluster Readiness Check ---\")\n",
    "# t-SNE is computationally expensive. We'll use a 10% sample.\n",
    "df_sample = df_eda.sample(frac=0.10, random_state=42)\n",
    "\n",
    "# We must scale the data *before* running t-SNE\n",
    "scaler_eda = StandardScaler()\n",
    "df_sample_scaled = scaler_eda.fit_transform(df_sample)\n",
    "\n",
    "print(f\"Running t-SNE on a sample of {len(df_sample)} customers... (This may take a minute)\")\n",
    "tsne_3d = TSNE(\n",
    "    n_components=3,\n",
    "    perplexity=30,\n",
    "    random_state=42,\n",
    "    n_iter=300,\n",
    "    init='pca'\n",
    ")\n",
    "tsne_components_3d = tsne_3d.fit_transform(df_sample_scaled)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_tsne_3d = pd.DataFrame(data = tsne_components_3d,\n",
    "                          columns = ['TSNE1', 'TSNE2', 'TSNE3'])\n",
    "print(\"t-SNE calculation complete.\")\n",
    "\n",
    "# --- 5.5.1 Plotting 3D t-SNE (Interactive) ---\n",
    "print(\"Generating interactive 3D t-SNE plot...\")\n",
    "fig_3d_tsne = px.scatter_3d(\n",
    "    df_tsne_3d,\n",
    "    x='TSNE1',\n",
    "    y='TSNE2',\n",
    "    z='TSNE3',\n",
    "    title='Interactive 3D t-SNE Projection (on 10% Sample)',\n",
    "    opacity=0.6,\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "fig_3d_tsne.update_traces(marker=dict(size=3))\n",
    "fig_3d_tsne.write_html(\"tsne_3d_plot.html\")\n",
    "print(\"✅ Interactive 3D t-SNE plot saved as 'tsne_3d_plot.html'.\")\n",
    "# This plot will appear in its own interactive window.\n",
    "fig_3d_tsne.show()\n",
    "\n",
    "# --- 5.5.2 Plotting 2D t-SNE (Static) ---\n",
    "print(\"\\nGenerating 2D t-SNE plot...\")\n",
    "# A final, separate figure for the 2D t-SNE\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(data=df_tsne_3d, x='TSNE1', y='TSNE2', alpha=0.6, s=20)\n",
    "plt.title('2D t-SNE Projection (Components 1 & 2)', fontsize=16)\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True)\n",
    "plt.savefig('tsne_2d_plot.png')\n",
    "print(\"✅ 2D t-SNE plot saved as 'tsne_2d_plot.png'.\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- ✅ EDA complete. All plots and stats saved. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmZ5cHZ1qFYb"
   },
   "source": [
    "### 6. Final Preprocessing for Modeling\n",
    "\n",
    "Our EDA in Block 5 definitively proved that our 8 features are highly skewed and have a wide range of scales. K-Means and DBSCAN are **distance-based algorithms**, meaning they are extremely sensitive to both of these issues.\n",
    "\n",
    "This block performs the two **mandatory transformations** to make our data ready for these models:\n",
    "\n",
    "1.  **Logarithmic Transformation:** We will apply a `np.log1p` transformation to our highly skewed features (e.g., `TotalMonetary`, `Recency`, `TotalOrders`). This \"squashes\" the extreme outliers and pulls the long tail in, converting the distribution to be more \"normal.\"\n",
    "2.  **Standard Scaling:** After logging, we will apply a `StandardScaler`. This final step rescales all 8 features so that they all have a **mean of 0 and a standard deviation of 1**.\n",
    "\n",
    "After this block, all features will contribute equally to the model, and we will have a clean, robust `df_scaled` dataset ready for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1762287500875,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "IEJaij-XSf2V",
    "outputId": "a5b349f3-7ad0-4fa6-c689-29fde6c2f741"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 6: Final Preprocessing for Modeling\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Original shape of data: {df_final.shape}\")\n",
    "\n",
    "# --- 6.1 Identify Features for Transformation ---\n",
    "# We use the 'df_eda' (X) and 'df_final' (X + y) from earlier\n",
    "# We will transform all features *except* our ratio, CancellationRate\n",
    "# Note: AOV can be negative if returns > purchases in a period,\n",
    "# but in our df_final (from df_purchases), min AOV should be > 0.\n",
    "# We will use log1p (log(x+1)) to handle any values of 0.\n",
    "\n",
    "# List of skewed features identified in EDA\n",
    "skewed_features = [\n",
    "    'Recency',\n",
    "    'CustomerTenure',\n",
    "    'TotalOrders',\n",
    "    'TotalMonetary',\n",
    "    'AverageOrderValue',\n",
    "    'AverageBasketSize',\n",
    "    'ProductDiversity'\n",
    "]\n",
    "\n",
    "# --- 6.2 Apply Logarithmic Transformation ---\n",
    "# We create a new dataframe for our preprocessed data\n",
    "df_processed = df_final.copy()\n",
    "\n",
    "print(f\"\\nApplying log transform to {len(skewed_features)} features...\")\n",
    "for col in skewed_features:\n",
    "    df_processed[col] = np.log1p(df_processed[col])\n",
    "\n",
    "print(\"Log transform complete.\")\n",
    "\n",
    "# --- 6.3 Apply Standard Scaling ---\n",
    "print(\"Applying standard scaling to all 8 features...\")\n",
    "# Set CustomerID as the index. It is an identifier, not a feature.\n",
    "df_processed = df_processed.set_index('CustomerID')\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "# This creates a NumPy array, which is what our models expect\n",
    "df_scaled = scaler.fit_transform(df_processed)\n",
    "\n",
    "print(\"✅ Data successfully logged and scaled.\")\n",
    "\n",
    "# --- 6.4 Inspection of Processed Data ---\n",
    "print(f\"\\nShape of final scaled data (NumPy array): {df_scaled.shape}\")\n",
    "print(\"This data is now ready for clustering.\")\n",
    "\n",
    "# We can also display the head of the *logged* data (before scaling)\n",
    "print(\"\\n--- Head of Log-Transformed Data (Before Scaling) ---\")\n",
    "display(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnC6PLW6ilEf"
   },
   "source": [
    "### 7. Modeling Part 1: K-Means Clustering\n",
    "\n",
    "Now that our data is fully preprocessed, we can begin the modeling phase. Our first approach is **K-Means Clustering**, a robust and popular algorithm.\n",
    "\n",
    "#### 7.1 Algorithm & Objective\n",
    "\n",
    "K-Means is a distance-based algorithm that partitions data into a pre-defined number of clusters (k). Its objective is to minimize the **Within-Cluster Sum of Squares (WCSS)**, also known as **inertia**. This is the sum of the squared distances between each data point and its assigned cluster's center (centroid). A lower WCSS means the clusters are more compact and well-defined.\n",
    "\n",
    "#### 7.2 Finding the Optimal 'k' (The Elbow Method)\n",
    "\n",
    "The main challenge with K-Means is that we must *tell it* how many clusters (k) to find. To find the optimal 'k', we will use the **Elbow Method**.\n",
    "\n",
    "* **What it does:** We will run the K-Means algorithm for a range of 'k' values (e.g., 1 to 10) and plot the WCSS (inertia) for each 'k'.\n",
    "* **How to read the plot:** As 'k' increases, the WCSS will always decrease. However, the \"optimal\" 'k' is found at the \"elbow\" — the point on the graph where the rate of decrease sharply flattens, resembling an arm. This point represents the best trade-off between minimizing WCSS and not having too many (over-fitted) clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "executionInfo": {
     "elapsed": 2712,
     "status": "ok",
     "timestamp": 1762287845839,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "6vRwFJIHg4cF",
    "outputId": "24c8cdbc-f121-4733-b0dd-a4ff526d3616"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 7: Modeling Part 1 - K-Means Elbow Method\n",
    "# =============================================================================\n",
    "\n",
    "# We will use the df_scaled (NumPy array) from Block 6\n",
    "# and the KMeans class imported in Block 1\n",
    "\n",
    "print(\"--- 7.1 Running K-Means Elbow Method ---\")\n",
    "\n",
    "# We will store the WCSS (inertia) for each k\n",
    "wcss = []\n",
    "k_range = range(1, 11)  # We will test k from 1 to 10 clusters\n",
    "\n",
    "for k in k_range:\n",
    "    # Initialize K-Means\n",
    "    # 'init'='k-means++' is a smart initialization technique\n",
    "    # 'n_init'='auto' (or 10) runs the algorithm multiple times to find the best result\n",
    "    # 'random_state'=42 ensures our results are reproducible\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n",
    "\n",
    "    # Fit the model to our scaled data\n",
    "    kmeans.fit(df_scaled)\n",
    "\n",
    "    # Append the inertia (WCSS) to our list\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "print(\"WCSS values calculated for k=1 to 10.\")\n",
    "\n",
    "# --- 7.2 Plot the Elbow Method Graph ---\n",
    "print(\"Generating Elbow Method plot...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, wcss, marker='o', linestyle='--')\n",
    "plt.title('K-Means Elbow Method', fontsize=16)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS (Inertia)')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.savefig('kmeans_elbow_plot.png')\n",
    "print(\"✅ kmeans_elbow_plot.png saved.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4LzodMvkBvm"
   },
   "source": [
    "### 7. Modeling Part 2: K-Means Execution & Cluster Assignment\n",
    "\n",
    "Based on the Elbow Method plot from the previous step, we can identify the optimal number of clusters (k). The \"elbow\" appears to be at **k=4** (or k=3, depending on the plot). This indicates that 4 clusters is the best balance between model complexity and explanatory power (WCSS).\n",
    "\n",
    "We will now re-run the K-Means algorithm with our chosen `k=4` and assign the resulting cluster labels back to our customer data.\n",
    "\n",
    "* **What it does:**\n",
    "    1.  Runs the `KMeans` algorithm on our `df_scaled` data with `n_clusters=4`.\n",
    "    2.  Assigns the resulting cluster label (0, 1, 2, or 3) to each customer.\n",
    "    3.  Adds these new labels back to both our `df_final` (original values) and `df_processed` (log-transformed values) DataFrames.\n",
    "* **Why we do this:** This is the core segmentation step. By adding the cluster labels back to our human-readable DataFrames, we can now move on to the most important part of the project: **Cluster Profiling**. This allows us to analyze the *characteristics* of each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1762288224298,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "Q7rMz6uxL6nz",
    "outputId": "9d531c56-7e39-459e-c5f2-e6315e87c15d"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 8: Modeling Part 2 - K-Means Execution\n",
    "# =============================================================================\n",
    "\n",
    "# We will use the df_scaled (NumPy array) from Block 6\n",
    "# and df_final / df_processed from Blocks 4 & 6\n",
    "\n",
    "# --- 8.1 Set Optimal k ---\n",
    "# *** ACTION REQUIRED: ***\n",
    "# Based on your elbow plot from Block 7, set the optimal 'k' here.\n",
    "# We will proceed with k=4 as a common example.\n",
    "OPTIMAL_K = 4\n",
    "\n",
    "print(f\"--- 8.1 Running K-Means with optimal k={OPTIMAL_K} ---\")\n",
    "\n",
    "# Initialize and run the final K-Means model\n",
    "kmeans = KMeans(n_clusters=OPTIMAL_K, init='k-means++', n_init=10, random_state=42)\n",
    "kmeans.fit(df_scaled)\n",
    "\n",
    "# Get the cluster labels for each customer\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# --- 8.2 Assign Cluster Labels Back to DataFrames ---\n",
    "# We add the labels to both our original and processed data for analysis\n",
    "\n",
    "# 1. Add to the log-transformed data (df_processed)\n",
    "df_processed['Cluster'] = cluster_labels\n",
    "\n",
    "# 2. Add to the original-value data (df_final)\n",
    "# We must be careful to match the index, as df_final has CustomerID as a column\n",
    "# df_processed has CustomerID as an index.\n",
    "df_final_clustered = df_final.set_index('CustomerID')\n",
    "df_final_clustered['Cluster'] = cluster_labels\n",
    "df_final_clustered = df_final_clustered.reset_index()\n",
    "\n",
    "print(\"✅ Cluster labels assigned back to DataFrames.\")\n",
    "\n",
    "# --- 8.3 Inspect the Results ---\n",
    "print(f\"\\n--- Cluster Distribution (Size of Each Segment) ---\")\n",
    "# Display the number of customers in each cluster\n",
    "print(df_final_clustered['Cluster'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n--- Head of Final Clustered Data (Original Values) ---\")\n",
    "display(df_final_clustered.head())\n",
    "\n",
    "print(\"\\n--- Head of Processed Clustered Data (Logged Values) ---\")\n",
    "display(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iywuwK7SodPS"
   },
   "source": [
    "### 8. Cluster Profiling & Interpretation\n",
    "\n",
    "This is the most important part of the entire analysis. We have successfully segmented our customers into 4 (or your `OPTIMAL_K`) distinct groups. Now, we must figure out *who these groups are*.\n",
    "\n",
    "#### 8.1 Objective\n",
    "\n",
    "Our goal is to create a \"persona\" for each cluster. We will do this by grouping all customers by their assigned cluster and calculating the average value for each of the 8 features.\n",
    "\n",
    "* **What it does:** We will use our `df_final_clustered` DataFrame (which contains the **original, human-readable values**) to calculate the mean of `Recency`, `CustomerTenure`, `TotalMonetary`, `CancellationRate`, etc., for each cluster.\n",
    "* **Why we do this:** This allows us to compare the clusters side-by-side. For example, we might find:\n",
    "    * **Cluster 0:** Has LOW Recency, HIGH TotalMonetary, HIGH TotalOrders... (These are our \"Champions\").\n",
    "    * **Cluster 1:** Has HIGH Recency, LOW TotalOrders, LOW Tenure... (These are our \"Lost Customers\").\n",
    "    * **Cluster 2:** Has LOW Recency, LOW Tenure, HIGH CancellationRate... (These are our \"At-Risk New Customers\").\n",
    "\n",
    "This profile is what we deliver to the business to build marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 860,
     "status": "ok",
     "timestamp": 1762289390226,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "F9iN3oH1Z04k",
    "outputId": "0bc66ba2-8dcd-48ec-bdc3-da3c14f46c29"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 9: Cluster Profiling - Statistical Analysis\n",
    "# =============================================================================\n",
    "\n",
    "# We will use the df_final_clustered DataFrame from Block 8,\n",
    "# as it contains the *original, unscaled* values, which are human-readable.\n",
    "\n",
    "print(\"--- 9.1 Calculating Cluster Profiles (Averages) ---\")\n",
    "\n",
    "# Group by the 'Cluster' label and calculate the mean for all 8 features\n",
    "# We select all feature columns + 'Cluster'\n",
    "profile_features = feature_columns + ['Cluster']\n",
    "df_profile = df_final_clustered[profile_features].groupby('Cluster').mean()\n",
    "\n",
    "# We can also add the 'size' of each cluster to this profile\n",
    "df_profile['ClusterSize'] = df_final_clustered['Cluster'].value_counts().sort_index()\n",
    "\n",
    "# Transpose (.T) the DataFrame for a much cleaner, vertical \"persona\" view\n",
    "df_profile_t = df_profile.T\n",
    "\n",
    "print(\"✅ Cluster profiling complete.\")\n",
    "\n",
    "# --- 9.2 Display the Cluster Profiles ---\n",
    "print(\"\\n--- Cluster Profile Table (Averages per Cluster) ---\")\n",
    "display(df_profile_t)\n",
    "\n",
    "# --- 9.3 Plot Cluster Sizes ---\n",
    "print(\"\\n--- 9.3 Generating Cluster Size Plot ---\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=df_final_clustered, x='Cluster', palette='viridis')\n",
    "plt.title('Customer Segment Size Distribution', fontsize=16)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.savefig('cluster_size_barchart.png')\n",
    "print(\"✅ cluster_size_barchart.png saved.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AR51CvU8qRQC"
   },
   "source": [
    "### 9. Cluster Profiling & Visualization\n",
    "\n",
    "This is the most critical stage of the analysis: **interpretation**. We have successfully segmented our customers into 4 (or your `OPTIMAL_K`) distinct groups. Now, we must figure out *who these groups are*.\n",
    "\n",
    "#### 9.1 Statistical Profiling\n",
    "\n",
    "Our first step is to create a statistical \"persona\" for each cluster.\n",
    "\n",
    "* **What it does:** We will group all customers by their assigned cluster and calculate the average value for each of our 8 features. We will use the `df_final_clustered` DataFrame, which contains the **original, human-readable values** (not the scaled ones).\n",
    "* **Why we do this:** This allows us to compare the clusters side-by-side. For example, we might find:\n",
    "    * **Cluster 0:** Has LOW `Recency`, HIGH `TotalMonetary`, HIGH `TotalOrders`... (These are our \"Champions\").\n",
    "    * **Cluster 1:** Has HIGH `Recency`, LOW `TotalOrders`, LOW `CustomerTenure`... (These are our \"Lost Customers\").\n",
    "\n",
    "This profile is what we deliver to the business to build marketing strategies.\n",
    "\n",
    "#### 9.2 Visual Profiling (Snake Plot)\n",
    "\n",
    "A table of averages can be hard to read. A **\"Snake Plot\"** is an advanced visualization that plots the *scaled* average value of each feature for all clusters on a single line graph.\n",
    "\n",
    "* **What it does:** It shows the \"profile\" of each cluster as a \"snake.\"\n",
    "* **Why we do this:** It makes it incredibly easy to see the differences. We can instantly spot which cluster is \"high\" on `TotalMonetary` but \"low\" on `Recency`. We will use our `df_processed` (log-transformed and scaled) data for this plot, as all features are on the same scale (mean of 0, std of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3115,
     "status": "ok",
     "timestamp": 1762289886786,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "rwsDtPK6Z_JE",
    "outputId": "6835f4ed-4a6c-40cb-a5b4-3d473d4e6049"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 9: Cluster Profiling & Visualization\n",
    "# =============================================================================\n",
    "\n",
    "# We will use df_final_clustered (original values) and df_processed (scaled values)\n",
    "\n",
    "# --- 9.1 Statistical Profiling ---\n",
    "print(\"--- 9.1 Calculating Cluster Profiles (Averages) ---\")\n",
    "\n",
    "# We use the original, unscaled values for human-readable interpretation\n",
    "profile_features = feature_columns + ['Cluster']\n",
    "df_profile = df_final_clustered[profile_features].groupby('Cluster').mean()\n",
    "\n",
    "# Add the 'ClusterSize' to our profile table\n",
    "df_profile['ClusterSize'] = df_final_clustered['Cluster'].value_counts().sort_index()\n",
    "\n",
    "# Transpose (.T) the DataFrame for a much cleaner, vertical \"persona\" view\n",
    "df_profile_t = df_profile.T\n",
    "\n",
    "print(\"✅ Cluster profiling complete.\")\n",
    "\n",
    "# --- 9.2 Display the Cluster Profiles ---\n",
    "print(\"\\n--- Cluster Profile Table (Averages per Cluster) ---\")\n",
    "# This table is the main result for defining our personas\n",
    "display(df_profile_t)\n",
    "\n",
    "# --- 9.3 Plot Cluster Sizes ---\n",
    "print(\"\\n--- 9.3 Generating Cluster Size Plot ---\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "# We use the 'df_final_clustered' which has the cluster assignments\n",
    "sns.countplot(data=df_final_clustered, x='Cluster', palette='viridis')\n",
    "plt.title('Customer Segment Size Distribution', fontsize=16)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.savefig('cluster_size_barchart.png')\n",
    "print(\"✅ cluster_size_barchart.png saved.\")\n",
    "plt.show()\n",
    "\n",
    "# --- 9.4 Visual Profiling (Snake Plot) ---\n",
    "print(\"\\n--- 9.4 Generating Snake Plot (Scaled Averages) ---\")\n",
    "\n",
    "# We use the *processed* (logged and scaled) dataframe for this\n",
    "# This ensures all features are on the same scale (z-score)\n",
    "df_profile_scaled = df_processed.groupby('Cluster').mean()\n",
    "\n",
    "# Melt the dataframe from wide to long format for plotting with seaborn\n",
    "df_snake_plot = pd.melt(df_profile_scaled.reset_index(),\n",
    "                        id_vars='Cluster',\n",
    "                        value_vars=feature_columns,\n",
    "                        var_name='Feature',\n",
    "                        value_name='ScaledValue')\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.lineplot(data=df_snake_plot, x='Feature', y='ScaledValue', hue='Cluster',\n",
    "             palette='viridis', marker='o', sort=False)\n",
    "plt.title('Snake Plot: Comparing Scaled Cluster Averages', fontsize=16)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Scaled Average Value (Z-Score)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1) # Add a 'mean' line\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_snake_plot.png')\n",
    "print(\"✅ cluster_snake_plot.png saved.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcOdDg5ltZyo"
   },
   "source": [
    "### 10. K-Means: Cluster Interpretation & Personas\n",
    "\n",
    "This is the final, most important step of our K-Means analysis. Using the profile table and snake plot from Block 9, we will now define *who* these customers are by giving each cluster a descriptive \"persona.\"\n",
    "\n",
    "* **Methodology:** We analyze the 8-feature averages for each cluster, comparing them to the other clusters to find their unique characteristics.\n",
    "\n",
    "#### **Cluster Persona Analysis:**\n",
    "\n",
    "* **Cluster 0: \"High-Value Churn Risk\" (Size: 1,185)**\n",
    "    * **Profile:** This group has a high Average Order Value ($715) and Basket Size (451 items), meaning they place large, valuable orders.\n",
    "    * **Problem:** They have a high Recency (126 days) and a very high Cancellation Rate (18%).\n",
    "    * **Interpretation:** These are valuable customers who are **fading away**. They haven't shopped in a long time and are unreliable, costing the business money with cancellations.\n",
    "    * **Strategy:** Target with personalized win-back campaigns, but be cautious due to their high cancellation habit.\n",
    "\n",
    "* **Cluster 1: \"Lost Customers\" (Size: 1,151)**\n",
    "    * **Profile:** This is the worst-performing group across almost every metric: *highest* Recency (174 days), *lowest* Orders, *lowest* Monetary value, and *lowest* Diversity.\n",
    "    * **Problem:** They have churned.\n",
    "    * **Interpretation:** These are low-value customers who stopped shopping almost 6 months ago.\n",
    "    * **Strategy:** Do not invest heavily. A single, automated \"we miss you\" email with a deep discount is the maximum effort.\n",
    "\n",
    "* **Cluster 2: \"Champions\" (Size: 1,212)**\n",
    "    * **Profile:** This is our **best and most valuable segment.** They are the top performers in every key category: *lowest* Recency (20 days), *highest* Tenure, *highest* Total Orders, *highest* Total Monetary value, and *highest* Product Diversity.\n",
    "    * **Problem:** Their only minor flaw is a medium-high Cancellation Rate (14%).\n",
    "    * **Interpretation:** These are our loyal, engaged, high-spending VIPs.\n",
    "    * **Strategy:** **Retain & Reward.** Offer loyalty perks, early access, and exclusive content. Do *not* use discounts. Investigate their cancellations to improve their experience.\n",
    "\n",
    "* **Cluster 3: \"New & Promising\" (Size: 790)**\n",
    "    * **Profile:** This group has the *lowest* Tenure (49 days), meaning they are brand new. They have good Recency (32 days) and the *lowest* Cancellation Rate (4%).\n",
    "    * **Problem:** Their spending and order counts are still low (as expected).\n",
    "    * **Interpretation:** These are our newest customers, and they are behaving well (low cancellations). They represent the future growth of the business.\n",
    "    * **Strategy:** **Nurture & Grow.** Onboard them with a welcome series. Encourage their second and third purchase with targeted follow-ups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEqop46ruLNz"
   },
   "source": [
    "### 11. Modeling Part 3: DBSCAN Clustering\n",
    "\n",
    "Now that we have a complete analysis from K-Means, we will run a second, more advanced model: **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**.\n",
    "\n",
    "#### 11.1 Algorithm & Objective\n",
    "\n",
    "DBSCAN is a powerful density-based model. Unlike K-Means, it does *not* require us to specify the number of clusters. Instead, it finds clusters based on how \"dense\" the data is.\n",
    "\n",
    "Its great advantage is that it can identify **noise points (outliers)**. K-Means forces every customer into a cluster, even if they don't fit. DBSCAN will label these outliers as a separate group (labeled \"-1\"), which is an extremely valuable business segment (e.g., \"Anomalous Customers,\" \"One-Time Bulk Buyers,\" or \"Fraudulent Accounts\").\n",
    "\n",
    "* **`eps` (Epsilon):** The maximum distance between two points for one to be considered in the \"neighborhood\" of the other. This is the most important parameter to tune.\n",
    "* **`min_samples`:** The minimum number of points required to form a dense \"core\" region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 1834,
     "status": "ok",
     "timestamp": 1762290886907,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "oCbGVwpYuMHT",
    "outputId": "369405ec-40d3-4708-b873-28b63f0f27c6"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 10: Modeling Part 3 - DBSCAN Clustering\n",
    "# =============================================================================\n",
    "\n",
    "# We will use the *same* df_scaled (NumPy array) from Block 6\n",
    "# This allows for a fair, direct comparison with our K-Means results\n",
    "\n",
    "print(\"--- 10.1 Running DBSCAN ---\")\n",
    "\n",
    "# --- 10.1.1 Tuning DBSCAN ---\n",
    "# Finding the right 'eps' is key. A common method is to find the 'knee'\n",
    "# in a k-nearest neighbors plot. For 8-dimensional data, this is complex.\n",
    "# A good starting point is to test values based on the data's properties.\n",
    "# Given our 8 features (all scaled to mean=0, std=1), a distance of\n",
    "# 2.5 - 3.5 is a reasonable starting point to test.\n",
    "# 'min_samples' is often set to 2 * number_of_dimensions\n",
    "MIN_SAMPLES = 2 * df_scaled.shape[1]  # 2 * 8 = 16\n",
    "EPS = 3.0  # This is a hyperparameter to tune. We'll start with 3.0\n",
    "\n",
    "dbscan = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES)\n",
    "dbscan_labels = dbscan.fit_predict(df_scaled)\n",
    "\n",
    "# --- 10.2 Analyze DBSCAN Results ---\n",
    "# The label '-1' is given to all \"noise\" points (outliers)\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "n_clustered = len(dbscan_labels) - n_noise\n",
    "\n",
    "print(f\"✅ DBSCAN clustering complete.\")\n",
    "print(f\"   Parameters: eps={EPS}, min_samples={MIN_SAMPLES}\")\n",
    "print(f\"\\n--- DBSCAN Results ---\")\n",
    "print(f\"Total Clusters Found: {n_clusters}\")\n",
    "print(f\"Clustered Customers:  {n_clustered} ({n_clustered/len(dbscan_labels):.1%})\")\n",
    "print(f\"Outlier Customers (-1): {n_noise} ({n_noise/len(dbscan_labels):.1%})\")\n",
    "\n",
    "# --- 10.3 Add DBSCAN Labels to DataFrames ---\n",
    "# We can now add these new labels to compare with our K-Means results\n",
    "df_final_clustered['DBSCAN_Cluster'] = dbscan_labels\n",
    "df_processed['DBSCAN_Cluster'] = dbscan_labels\n",
    "\n",
    "print(\"\\n--- DBSCAN Cluster Size Distribution ---\")\n",
    "print(df_final_clustered['DBSCAN_Cluster'].value_counts())\n",
    "\n",
    "print(\"\\n--- Head of Final Data with Both Cluster Labels ---\")\n",
    "display(df_final_clustered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iREBVhxMu0f4"
   },
   "source": [
    "### 11. Modeling Part 4: DBSCAN Hyperparameter Tuning\n",
    "\n",
    "The DBSCAN run in Block 10 failed, identifying 99.7% of all customers as a single cluster. This is a classic symptom of an `eps` (Epsilon) value that is far too large.\n",
    "\n",
    "To find the optimal `eps`, we cannot \"guess.\" We must use a systematic method. The standard approach is to calculate the distance to the k-th nearest neighbor for every point, where 'k' is our `min_samples` (which we set to 16).\n",
    "\n",
    "* **What it does:** We will calculate the distance for every customer to its 16th nearest neighbor, sort these distances, and plot them.\n",
    "* **How to read the plot:** This plot will show a \"knee\" or \"elbow.\" The y-value of this \"knee\" is the optimal `eps`. It represents the point where the distance between points suddenly jumps, which is the exact definition of a cluster border."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "executionInfo": {
     "elapsed": 712,
     "status": "ok",
     "timestamp": 1762291052065,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "WBYA-DcJu1S6",
    "outputId": "1e52c53a-0788-49f6-debf-7ce322808003"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 11: DBSCAN Hyperparameter Tuning (Epsilon Knee Plot)\n",
    "# =============================================================================\n",
    "\n",
    "# We will use the same df_scaled data\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# --- 11.1 Calculate K-NN Distances ---\n",
    "# We use min_samples (16) as our 'k' (n_neighbors)\n",
    "# MIN_SAMPLES was set to 16 in the previous block\n",
    "k_neighbors = MIN_SAMPLES\n",
    "\n",
    "print(f\"Calculating distances for {k_neighbors}-Nearest Neighbors...\")\n",
    "\n",
    "# Initialize and fit the model\n",
    "neighbors = NearestNeighbors(n_neighbors=k_neighbors)\n",
    "neighbors.fit(df_scaled)\n",
    "\n",
    "# Find the distances to the k-th neighbor for all points\n",
    "distances, indices = neighbors.kneighbors(df_scaled)\n",
    "\n",
    "# Get *only* the distance to the k-th (16th) neighbor (the last one)\n",
    "k_distances = distances[:, k_neighbors-1]\n",
    "\n",
    "# Sort the distances\n",
    "sorted_distances = np.sort(k_distances)\n",
    "\n",
    "print(\"Distances calculated and sorted.\")\n",
    "\n",
    "# --- 11.2 Plot the Epsilon Knee ---\n",
    "print(\"Generating Epsilon 'Knee' Plot...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sorted_distances)\n",
    "plt.title('Epsilon (eps) \"Knee\" Plot', fontsize=16)\n",
    "plt.xlabel('Customers (Sorted by Distance)')\n",
    "plt.ylabel(f'Distance to {k_neighbors}-th Neighbor (eps)')\n",
    "plt.grid(True)\n",
    "plt.savefig('dbscan_knee_plot.png')\n",
    "print(\"✅ dbscan_knee_plot.png saved.\")\n",
    "plt.show()\n",
    "\n",
    "# --- 11.3 How to Read This Plot ---\n",
    "print(\"\\n--- How to Read This Plot ---\")\n",
    "print(\"Look for the 'knee' in the plot (the point of maximum curvature).\")\n",
    "print(\"The Y-axis value at that 'knee' is your new, optimal 'eps' value.\")\n",
    "print(\"It will likely be much smaller than 3.0 (e.g., somewhere between 1.5 and 2.5).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8_ejkzKvvoV"
   },
   "source": [
    "### 11. Modeling Part 5: Re-running DBSCAN with Optimal `eps`\n",
    "\n",
    "The \"Epsilon Knee Plot\" from the previous block gives us a clear, data-driven value for our `eps` hyperparameter.\n",
    "\n",
    "* **Finding:** After re-examining the plot, the \"knee\" (the point of maximum curvature where the slope first changes dramatically) is located at a Y-axis value of approximately **1.8**.\n",
    "* **Interpretation:** This means the optimal distance to define a \"neighborhood\" is 1.8. Our previous value of 3.0 (and 2.4) was too large, causing the model to see most points as one giant cluster. This new, smaller `eps` should result in a much more granular and accurate segmentation.\n",
    "* **Action:** We will now re-run DBSCAN using our new, correct value of `eps = 1.8` and the same `min_samples = 16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1762291308277,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "2t4-uJ1Uvk58",
    "outputId": "4ec2ca6c-aef9-4762-f5e5-ff1f34e74e03"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 12: Re-running DBSCAN with Optimal Epsilon (1.8)\n",
    "# =============================================================================\n",
    "\n",
    "# We will use the *same* df_scaled (NumPy array) from Block 6\n",
    "\n",
    "# --- 12.1 Set Optimal Parameters ---\n",
    "# From our corrected knee plot analysis\n",
    "OPTIMAL_EPS = 1.8\n",
    "MIN_SAMPLES = 16  # (This remains 2 * number_of_features)\n",
    "\n",
    "print(f\"--- 12.1 Re-running DBSCAN with optimal parameters ---\")\n",
    "print(f\"   Parameters: eps={OPTIMAL_EPS}, min_samples={MIN_SAMPLES}\")\n",
    "\n",
    "# --- 12.2 Run the final DBSCAN model ---\n",
    "dbscan_final = DBSCAN(eps=OPTIMAL_EPS, min_samples=MIN_SAMPLES)\n",
    "dbscan_labels_final = dbscan_final.fit_predict(df_scaled)\n",
    "\n",
    "# --- 12.3 Analyze New DBSCAN Results ---\n",
    "n_clusters_final = len(set(dbscan_labels_final)) - (1 if -1 in dbscan_labels_final else 0)\n",
    "n_noise_final = list(dbscan_labels_final).count(-1)\n",
    "n_clustered_final = len(dbscan_labels_final) - n_noise_final\n",
    "\n",
    "print(f\"\\n✅ DBSCAN (final run) complete.\")\n",
    "print(f\"\\n--- Final DBSCAN Results ---\")\n",
    "print(f\"Total Clusters Found: {n_clusters_final}\")\n",
    "print(f\"Clustered Customers:  {n_clustered_final} ({n_clustered_final/len(dbscan_labels_final):.1%})\")\n",
    "print(f\"Outlier Customers (-1): {n_noise_final} ({n_noise_final/len(dbscan_labels_final):.1%})\")\n",
    "\n",
    "# --- 12.4 Add Final DBSCAN Labels to DataFrames ---\n",
    "df_final_clustered['DBSCAN_Cluster_Final'] = dbscan_labels_final\n",
    "df_processed['DBSCAN_Cluster_Final'] = dbscan_labels_final\n",
    "\n",
    "print(\"\\n--- Final DBSCAN Cluster Size Distribution ---\")\n",
    "# This output will show us the new, better cluster sizes\n",
    "print(df_final_clustered['DBSCAN_Cluster_Final'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBSU7i6ZwUuw"
   },
   "source": [
    "### 12. Model Validation: Feature Importance for K-Means\n",
    "\n",
    "Our K-Means model (Block 8) found 4 clusters, but our DBSCAN model (Block 12) found only 1. This is a critical conflict. We must now **validate our K-Means results**.\n",
    "\n",
    "Are the 4 clusters meaningful, or are they an artificial result of the algorithm?\n",
    "\n",
    "To find out, we will use the **Permutation Importance** test.\n",
    "\n",
    "#### 12.1 Methodology (The \"Shuffle Test\")\n",
    "\n",
    "1.  **Create a \"Target\":** We will use the 4 cluster labels from our K-Means model (0, 1, 2, 3) as our \"target variable.\"\n",
    "2.  **Train a \"Surrogate\" Model:** We will train a new, supervised classifier (a Random Forest) to *predict* which cluster a customer belongs to, based on our 8 scaled features.\n",
    "3.  **Run the \"Shuffle Test\":** We will then test this Random Forest's accuracy. We will shuffle *one feature* at a time (e.g., shuffle all `Recency` values) and see how much the model's accuracy drops.\n",
    "\n",
    "If shuffling a feature (like `Recency`) causes the model's accuracy to collapse, it proves that `Recency` was **critically important** for building those clusters. If shuffling a feature does nothing, that feature was just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "executionInfo": {
     "elapsed": 5956,
     "status": "ok",
     "timestamp": 1762291628519,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "_uAcCKW6wVhM",
    "outputId": "638e38aa-bc2a-469a-d84f-c6ee2363e49e"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 13: Feature Importance Analysis (Validating K-Means)\n",
    "# =============================================================================\n",
    "\n",
    "# We need a classifier and the importance function\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"--- 13.1 Validating K-Means with a Surrogate Model ---\")\n",
    "\n",
    "# --- 1. Define our X and y ---\n",
    "X = df_scaled  # Our 8 scaled features\n",
    "# 'cluster_labels' is the NumPy array from our K-Means run in Block 8\n",
    "y = cluster_labels\n",
    "\n",
    "# --- 2. Split into Training and Testing sets ---\n",
    "# This is crucial for a valid permutation importance test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# --- 3. Train the Random Forest \"Surrogate\" Model ---\n",
    "print(\"Training Random Forest classifier to predict K-Means clusters...\")\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Check how well it learned the clusters\n",
    "accuracy = rf_classifier.score(X_test, y_test)\n",
    "print(f\"Surrogate Model Accuracy: {accuracy:.2%}\")\n",
    "print(\"(A high accuracy > 95% means the clusters are well-defined by the features)\")\n",
    "\n",
    "# --- 4. Run the \"Shuffle Test\" (Permutation Importance) ---\n",
    "print(\"\\nRunning Permutation Importance (Shuffle Test)...\")\n",
    "# We run this on the *test* set to get an unbiased view\n",
    "result = permutation_importance(\n",
    "    rf_classifier, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- 5. Organize and Plot the Results ---\n",
    "#\n",
    "# *** THIS IS THE FIX: ***\n",
    "# We use our 'feature_columns' variable, which we created in Block 4.\n",
    "# It contains the correct, clean list of our 8 feature names.\n",
    "#\n",
    "feature_names = feature_columns\n",
    "\n",
    "# Create a clean DataFrame of the results\n",
    "importance_df = pd.DataFrame(\n",
    "    result.importances_mean,\n",
    "    index=feature_names,\n",
    "    columns=['Importance']\n",
    ").sort_values(by='Importance', ascending=True)\n",
    "\n",
    "print(\"\\n--- Feature Importance for K-Means Clusters ---\")\n",
    "print(importance_df)\n",
    "\n",
    "# --- 6. Plot the results ---\n",
    "print(\"\\nGenerating Feature Importance plot...\")\n",
    "plt.figure(figsize=(10, 7))\n",
    "importance_df.plot(kind='barh', legend=False)\n",
    "plt.title('K-Means Feature Importance (Permutation Test)', fontsize=16)\n",
    "plt.xlabel('Importance (Drop in Model Accuracy)')\n",
    "plt.ylabel('Feature')\n",
    "plt.grid(True, axis='x', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.savefig('kmeans_feature_importance.png')\n",
    "print(\"✅ kmeans_feature_importance.png saved.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpxULC-lxxyu"
   },
   "source": [
    "### 13. Analysis of Feature Importance (The \"Shuffle Test\")\n",
    "\n",
    "This block is the final validation of our entire K-Means model. The results are extremely positive and provide two critical insights.\n",
    "\n",
    "#### Insight 1: Our K-Means Clusters are \"Real\" and Meaningful\n",
    "\n",
    "* **`Surrogate Model Accuracy: 94.62%`**\n",
    "* **Interpretation:** This is an outstanding result. It means our \"surrogate\" Random Forest model was able to predict which cluster a customer belonged to with ~95% accuracy, using only our 8 features.\n",
    "* **Why it Matters:** This proves that our 4 K-Means clusters are **not** an artificial illusion. They are well-defined, mathematically separable groups that are strongly defined by our features. This gives us high confidence in our 4 personas.\n",
    "\n",
    "#### Insight 2: `CustomerTenure` is the Most Important Feature\n",
    "\n",
    "* **`Importance Rankings:`**\n",
    "    1.  **`CustomerTenure` (0.17)**\n",
    "    2.  `Recency` (0.11)\n",
    "    3.  `TotalMonetary` (0.11)\n",
    "    4.  `TotalOrders` (0.09)\n",
    "* **Interpretation:** The \"shuffle test\" shows that `CustomerTenure` (our advanced, non-RFM feature) was the **single most important factor** in defining the clusters. This is a huge win for our 8-feature model. `Recency` and `TotalMonetary` are also (predictably) very important.\n",
    "* **Why it Matters:** This proves our 8-feature model is **not overfitting**. We didn't just add noise; we added meaningful \"signals.\" It also validates our K-Means personas. For example, our \"Champions\" vs. \"New & Promising\" clusters were separated *primarily* by their `CustomerTenure`.\n",
    "\n",
    "This test also explains why DBSCAN \"failed.\" DBSCAN looks for *density*, but our clusters are defined by a complex interplay of 8 different features (led by `Tenure`), not just density. This confirms that **K-Means was the correct model for this problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwjVnyEpxyku"
   },
   "source": [
    "### 14. Project Conclusion & Final Recommendations\n",
    "\n",
    "This notebook successfully performed an end-to-end customer segmentation analysis, transforming raw, transactional data into four distinct, actionable customer personas.\n",
    "\n",
    "#### Summary of Findings:\n",
    "\n",
    "1.  **Advanced Model Justified:** Our 8-feature \"Customer DNA\" profile (which included `CustomerTenure`, `CancellationRate`, etc.) was validated by a Permutation Importance test. The test proved our clusters were meaningful (94.6% accuracy) and that our new features (especially `CustomerTenure`) were the most important drivers in segmenting customers.\n",
    "\n",
    "2.  **K-Means Was the Correct Model:** K-Means successfully identified 4 center-based clusters. DBSCAN (a density-based model) correctly identified one single, dense \"blob,\" proving that our customers are not separated by density but by their complex behavioral profiles.\n",
    "\n",
    "3.  **Four Actionable Segments Identified:** We have successfully profiled and named our 4 K-Means clusters:\n",
    "    * **Cluster 2: \"Champions\" (1,212 customers):** Our loyal, high-spending VIPs. (Strategy: **Retain & Reward**)\n",
    "    * **Cluster 3: \"New & Promising\" (790 customers):** Our new, well-behaved customers. (Strategy: **Nurture & Grow**)\n",
    "    * **Cluster 0: \"High-Value Churn Risk\" (1,185 customers):** High-value but high-cancellation customers who are fading away. (Strategy: **Win-Back & Investigate**)\n",
    "    * **Cluster 1: \"Lost Customers\" (1,151 customers):** Low-value, churned customers. (Strategy: **Do Not Invest**)\n",
    "\n",
    "#### Future Work & Recommendations:\n",
    "\n",
    "* **Deeper Dive into \"Churn Risk\":** Cluster 0 is our biggest risk and opportunity. A follow-up analysis should investigate *why* their `CancellationRate` is so high.\n",
    "* **Predictive Modeling:** This project was descriptive (\"What do our segments look like?\"). The next step is a predictive project: \"Can we build a model to predict which new customers (Cluster 3) will become 'Champions' (Cluster 2)?\"\n",
    "* **Deliver to Marketing:** These 4 segments are ready to be used. They can now be exported and used to build 4 distinct, personalized marketing campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1762292255344,
     "user": {
      "displayName": "Anuj Goel",
      "userId": "11989121148129746440"
     },
     "user_tz": -330
    },
    "id": "N53SBrxbx04o",
    "outputId": "9e0cc41a-7a1b-4bce-c355-71242c05d528"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Block 14: Export Final Segments\n",
    "# =============================================================================\n",
    "\n",
    "# We use the 'df_final_clustered' DataFrame from Block 8\n",
    "# It contains the CustomerID and the final K-Means 'Cluster' label\n",
    "\n",
    "print(\"--- 14.1 Preparing Final Export ---\")\n",
    "\n",
    "# We can also add the 'Persona' name based on our analysis from Block 10\n",
    "# This makes the file even more valuable to the business.\n",
    "persona_map = {\n",
    "    0: \"High-Value Churn Risk\",\n",
    "    1: \"Lost Customers\",\n",
    "    2: \"Champions\",\n",
    "    3: \"New & Promising\"\n",
    "}\n",
    "\n",
    "# Add a new 'Persona' column\n",
    "df_final_clustered['Persona'] = df_final_clustered['Cluster'].map(persona_map)\n",
    "\n",
    "# Select the columns for the final export\n",
    "df_export = df_final_clustered[['CustomerID', 'Cluster', 'Persona']]\n",
    "\n",
    "# --- 14.2 Save to CSV ---\n",
    "output_filename = \"customer_segments.csv\"\n",
    "try:\n",
    "    df_export.to_csv(output_filename, index=False)\n",
    "    print(f\"\\n✅ Successfully exported {len(df_export)} customers to '{output_filename}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving file: {e}\")\n",
    "\n",
    "# --- 14.3 Display Final Exportable Data ---\n",
    "print(\"\\n--- Head of Final Export File ---\")\n",
    "display(df_export.head())\n",
    "\n",
    "print(\"\\n\\n--- PROJECT COMPLETE ---\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNpC8oNtAj5GO80N1ctAAkA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
